{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d55d49-6456-4d5e-9f80-6aca6bbe1318",
   "metadata": {},
   "source": [
    "## Temporal Link Prediction in Co-Authorship Networks Using Graph Embedding Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e70926-74de-4999-9952-a9d520fad718",
   "metadata": {},
   "source": [
    "Authors: Mohammad Ghassemi (ghassem3@msu.edu), Sanaz Hasanzadeh, Soroush Ziaeinejad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a68c7-81ab-41a8-a548-a29078e0d998",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142f4f89-d2f6-4c23-812d-49116a776811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dynamicgem\n",
    "import csv\n",
    "#import DynamicGEM\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "#from dynamicgem.embedding.dynAERNN import DynAERNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31dad40-dd68-4a5a-9b3b-e88ee9ce0e28",
   "metadata": {},
   "source": [
    "### Step 2: Import required Data and Extract Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394d6d47-0696-4497-92fb-a0f3eead5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Link_Extraction(path):\n",
    "    \n",
    "    data_1 = [] #each row corresponds to one paper, containing names of all authors of the paper (seperated by \";\")\n",
    "    \n",
    "    with open(path, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            data_1.append(row[1])\n",
    "            \n",
    "        data_1[1:] #to remove the column name of dataset\n",
    "        \n",
    "    data_2 = [] #each row will be one paper's authors list, seperated by \",\"\n",
    "    \n",
    "    for i in range(len(data_1)):\n",
    "        data_2.append(data_1[i].split('; '))\n",
    "\n",
    "    authors_list = set() #each row is one author from data_2 list\n",
    "    for i in range(len(data_2)):\n",
    "        for j in range(len(data_2[i])):\n",
    "            authors_list.add(data_2[i][j])\n",
    "            \n",
    "    #authors_list.remove('')\n",
    "    \n",
    "    authors_comb = [] #each row will be all possible combinations C(data_2[i], 2) based on who published with who (edges)\n",
    "    for i in range(len(data_2)):\n",
    "        authors_comb.append(list(combinations(data_2[i], 2)))\n",
    "\n",
    "    # each row will be exactly one edge\n",
    "    links = set()\n",
    "    for i in range(len(authors_comb)):\n",
    "        for j in range(len(authors_comb[i])):\n",
    "            links.add(authors_comb[i][j])\n",
    "\n",
    "    links = list(links) \n",
    "    \n",
    "    return authors_list, links\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881131d-1a07-4ca5-8302-c62a550eef63",
   "metadata": {},
   "source": [
    "#### Step3: Call \"Link_Extraction\" function on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1024cd1d-bd34-4c97-bab4-68cd3ab38f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_list_2015, links_2015 = Link_Extraction('data/CSV/REPORTER_PUB_C_2015.csv')\n",
    "\n",
    "authors_list_2016, links_2016 = Link_Extraction('data/CSV/REPORTER_PUB_C_2016.csv')\n",
    "\n",
    "authors_list_2017, links_2017 = Link_Extraction('data/CSV/REPORTER_PUB_C_2017.csv')\n",
    "\n",
    "authors_list_2018, links_2018 = Link_Extraction('data/CSV/REPORTER_PUB_C_2018.csv')\n",
    "\n",
    "authors_list_2019, links_2019 = Link_Extraction('data/CSV/REPORTER_PUB_C_2019.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e13ff2c-380d-4980-8445-42ae28e2f3b3",
   "metadata": {},
   "source": [
    "#### 4- Intersection of all Years' Author-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2013a34-b2c7-442f-b0ef-d58b0d28ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_intersect = list(set.intersection(authors_list_2015, authors_list_2016, authors_list_2017, authors_list_2018, authors_list_2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b7e2c-29c7-4859-9f20-b6ce25e2a2c1",
   "metadata": {},
   "source": [
    "#### 5- Generate Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a74642-bedc-476a-80c7-f841efc0a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Graph_Generate(links, authors_intersect):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    all_links = filter(lambda c: c[0] in authors_intersect and c[1] in authors_intersect, links)\n",
    "    list_all_links = list(all_links)\n",
    "    #print(\"Num of Links: %float\" %len(list_all_links))\n",
    "    \n",
    "    for i in range(len(list_all_links)):\n",
    "        G.add_edge(list_all_links[i][0], list_all_links[i][1])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce51d6d-bdcd-4b82-9031-a081a7cb0d39",
   "metadata": {},
   "source": [
    "#### 6- Call Graph_Generate Function for each Year, Save Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3fa04-fac3-4d60-afe0-99fe665304ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_2015 = Graph_Generate(links_2015, authors_intersect)\n",
    "#print('2015 Graph is Generated')\n",
    "G_2016 = Graph_Generate(links_2016, authors_intersect)\n",
    "#print('2016 Graph is Generated')\n",
    "G_2017 = Graph_Generate(links_2017, authors_intersect)\n",
    "#print('2017 Graph is Generated')\n",
    "G_2018 = Graph_Generate(links_2018, authors_intersect)\n",
    "#print('2018 Graph is Generated')\n",
    "G_2019 = Graph_Generate(links_2019, authors_intersect)\n",
    "#print('2019 Graph is Generated')\n",
    "\n",
    "nx.write_pajek(G_2015, \"/data/Graphs/G-2015.net\")\n",
    "nx.write_pajek(G_2016, \"/data/Graphs/G-2016.net\")\n",
    "nx.write_pajek(G_2017, \"/data/Graphs/G-2017.net\")\n",
    "nx.write_pajek(G_2018, \"/data/Graphs/G-2018.net\")\n",
    "nx.write_pajek(G_2019, \"/data/Graphs/G-2019.net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b532ed4-c2f5-4218-b685-0c4c219ef52d",
   "metadata": {},
   "source": [
    "#### 7- Temporal Embedding\n",
    "Output is a npy array: (4, 62641, 128)\n",
    "first matrix is embeding for 2016 based on 2015\n",
    "second one is embeding for 2017 based on 2016 and 2015\n",
    "third one is embeding for 2018 based on 2017, 2016, and 2015\n",
    "fourth one is embeding for 2019 based on 2018, 2017, 2016, and 2015\n",
    "\n",
    "For any series of graphs the output will be in shape of (number of graphs-1, number of nodes, embedding dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690adae-13c7-413f-a328-f7e500bc5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle5 as pickle\n",
    "from dynamicgem.embedding.dynAERNN import DynAERNN\n",
    "\n",
    "# Set Variables\n",
    "dim_emb = 128\n",
    "lookback = 1\n",
    "batchNumber = 200\n",
    "graphPath = '../data/Graphs/'\n",
    "\n",
    "# To have sorted embedding in the Output\n",
    "def node_sorter(graph):\n",
    "    H = nx.Graph()\n",
    "    new_nodes = sorted(graph.nodes(data=True))\n",
    "    H.add_nodes_from(new_nodes)\n",
    "    H.add_edges_from(graph.edges(data=True))\n",
    "    return H\n",
    "\n",
    "graphlist = sorted(glob.glob(f'{graphPath}/*.net'))\n",
    "graphs = []\n",
    "for i in graphlist:\n",
    "    graphs.append(node_sorter(pickle.load(open(i, \"rb\"))))\n",
    "\n",
    "length = len(graphs)\n",
    "embedding = DynAERNN(d=dim_emb,\n",
    "                     beta=5,\n",
    "                     n_prev_graphs=lookback,\n",
    "                     nu1=1e-6,\n",
    "                     nu2=1e-6,\n",
    "                     n_aeunits=[500, 300],\n",
    "                     n_lstmunits=[500, dim_emb],\n",
    "                     rho=0.3,\n",
    "                     n_iter=25,\n",
    "                     xeta=1e-3,\n",
    "                     n_batch=batchNumber,\n",
    "                     modelfile=['enc_model_dynAERNN.json',\n",
    "                                'GEL/dec_model_dynAERNN.json'],\n",
    "                     weightfile=['GEL/enc_weights_dynAERNN.hdf5',\n",
    "                                 'GEL/dec_weights_dynAERNN.hdf5'],\n",
    "                     savefilesuffix=\"testing\")\n",
    "\n",
    "embs = []\n",
    "t1 = time.clock()\n",
    "for temp_var in range(lookback + 1, length + 1):\n",
    "    emb, _ = embedding.learn_embeddings(graphs[:temp_var])\n",
    "    print('emb type: ', type(emb))\n",
    "    embs.append(emb)\n",
    "embs = np.asarray(embs)\n",
    "e = embs.round(decimals=3)\n",
    "\n",
    "print('embs shape: ', embs)\n",
    "print(embedding._method_name + ':\\n\\tTraining time: %f' % (time.clock() - t1))\n",
    "np.save(f'{graphPath}/embeddeds_dim{dim_emb}_batchNumber{batchNumber}_length{length}_lookback{lookback}.npy', embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1adf8cc-cbd7-4cd3-bb07-b39cf0f1d4da",
   "metadata": {},
   "source": [
    "### Working with Embedded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800c499f-2320-4afb-b18b-3051c415f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pickle5 as pickle\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234961d4-008d-42c9-b861-922e3891e771",
   "metadata": {},
   "source": [
    "### 1- Load embedded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35a060c-0a39-48dd-91c4-3625d1dc211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embeddings = np.load('data/EMBEDDING/embeddeds_dim128_batchNumber200_length5_lookback1.npy')\n",
    "\n",
    "# We will predict 2019 collaborations based on 2018 and 2017 (these data is based on previous years, so prediction is based on\n",
    "# all previous years 2015, 2016, 2017, and 2018)\n",
    "embed2018 = embeddings[2]\n",
    "embed2017 = embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f9ad9c-dcae-4800-b1a3-64b9c8853365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have sorted embedding in Output\n",
    "def node_sorter(graph):\n",
    "    H = nx.Graph()\n",
    "    new_nodes = sorted(graph.nodes(data=True))\n",
    "    H.add_nodes_from(new_nodes)\n",
    "    H.add_edges_from(graph.edges(data=True))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917cf48e-d68b-419d-94e8-a7f56d54a822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes in all these graphs are their intersection, so nose_set is the same for all years\n",
    "\n",
    "graph_2019 = nx.read_pajek('data/Graphs/pajek_graph19.net')\n",
    "SortedNodeG2019 = node_sorter(graph_2019)\n",
    "node_emb_dict_2018 = {list(SortedNodeG2019.nodes())[i] : embed2018[i] for i in range(len(SortedNodeG2019.nodes()))}\n",
    "node_emb_dict_2017 = {list(SortedNodeG2019.nodes())[i] : embed2017[i] for i in range(len(SortedNodeG2019.nodes()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d309335-5890-4915-bfe3-a6df745213bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "node_emb_dict18 = list(node_emb_dict_2018)\n",
    "#node_emb_dict18[0]\n",
    "# samp_size = 20000\n",
    "# all_velbased_loc = []\n",
    "# for i in range(samp_size):\n",
    "#     all_velbased_loc.append(2*node_emb_dict_2018[i] - node_emb_dict_2017[i])\n",
    "# node_emb_dict18[1]\n",
    "#     temp.append(2*node_emb_dict_2018[f]-node_emb_dict_2017[f])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "628cddba-58ba-4535-98aa-bea27af29d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_size = 20001\n",
    "names = []\n",
    "for i in range(samp_size):\n",
    "    names.append(node_emb_dict18[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2999470e-fd18-4b7e-a12f-949abe795c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_velbased_loc = []\n",
    "for i in range(samp_size):\n",
    "     all_velbased_loc.append(2*node_emb_dict_2018[names[i]] - node_emb_dict_2017[names[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "704ca15b-80fe-4672-b042-a54ac8c1b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "ww, hh = 100, 200\n",
    "batch = [[0 for x in range(ww)] for y in range(hh)]\n",
    "s = 0\n",
    "stp = 100\n",
    "e = 100\n",
    "for b in range(hh):\n",
    "    batch[b][:] = all_velbased_loc[s: e]\n",
    "    s = e\n",
    "    e = e + stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dcd683-a5ff-4944-84ac-b7070a1d4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a list containing h lists, each of w items, all set to 0\n",
    "w, h = samp_size, samp_size\n",
    "Matrix = [[0 for x in range(w)] for y in range(h)]\n",
    "\n",
    "c = 0\n",
    "rs_samp = 20000\n",
    "\n",
    "for i in range(hh):\n",
    "    for j in range(hh):\n",
    "        for k in range(ww):\n",
    "            for l in range(ww):\n",
    "                a = batch[i][k].reshape(-1, 1)\n",
    "                b = batch[j][l].reshape(-1, 1)\n",
    "                temp = cosine_similarity(a, b)\n",
    "                row = int(c/rs_samp)\n",
    "                hight = c%rs_samp\n",
    "                Matrix[row][hight]\n",
    "                c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34730f-9770-4870-8525-5fd9d0ab4b58",
   "metadata": {},
   "source": [
    "### 2- Generate Train/ Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0493854b-5245-4d5f-b862-25eea1a124bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of negative samples is 4 times positive samples\n",
    "\n",
    "num_pos_samp = len(SortedNodeG2019.edges())\n",
    "num_neg_samp = 4*num_pos_samp\n",
    "\n",
    "nodeList = list(SortedNodeG2019.nodes())\n",
    "\n",
    "neg_set = []\n",
    "while len(neg_set) < num_neg_samp:\n",
    "    \n",
    "    first = random.randint(0, len(SortedNodeG2019.nodes())-1)\n",
    "    second = random.randint(0, len(SortedNodeG2019.nodes())-1)\n",
    "\n",
    "    f = nodeList[first]\n",
    "    s = nodeList[second]\n",
    "                            \n",
    "    if SortedNodeG2019.has_edge(f, s) == False:\n",
    "        neg_set.append((f, s))\n",
    "        \n",
    "edgeList = list(SortedNodeG2019.edges())\n",
    "\n",
    "# generate train/test data for baseline\n",
    "pos_samp = []\n",
    "for i in range(len(edgeList)):\n",
    "    temp = []\n",
    "    f = edgeList[i][0]\n",
    "    s = edgeList[i][1]\n",
    "    temp.append(f)\n",
    "    temp.append(s)\n",
    "    temp.append(node_emb_dict_2018[f])\n",
    "    temp.append(node_emb_dict_2018[s])\n",
    "    temp.append(1)\n",
    "    pos_samp.append(temp)\n",
    "    \n",
    "neg_samp = []\n",
    "for i in range(len(neg_set)):\n",
    "    temp = []\n",
    "    f = neg_set[i][0]\n",
    "    s = neg_set[i][1]\n",
    "    temp.append(f)\n",
    "    temp.append(s)\n",
    "    temp.append(node_emb_dict_2018[f])\n",
    "    temp.append(node_emb_dict_2018[s])\n",
    "    temp.append(-1)\n",
    "    neg_samp.append(temp)   \n",
    "    \n",
    "# positive and negative samples in a unified list\n",
    "samp = []\n",
    "for i in pos_samp:\n",
    "  samp.append(i)\n",
    "for i in neg_samp:\n",
    "  samp.append(i)    \n",
    "\n",
    "shuf_samp = random.sample(samp, len(samp))\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(shuf_samp)):\n",
    "    x.append(np.concatenate((shuf_samp[i][2], shuf_samp[i][3]), axis=None))\n",
    "    y.append(shuf_samp[i][4])\n",
    "    \n",
    "loc_xtrain, loc_xtest, loc_ytrain, loc_ytest = train_test_split(x, y, \n",
    "                                                test_size = 0.3, \n",
    "                                                random_state = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ed8d91-9ba1-480c-a1ae-69e04c0affb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train/test data for velocity\n",
    "\n",
    "vel_pos_samp = []\n",
    "for i in range(len(edgeList)):\n",
    "    temp = []\n",
    "    f = edgeList[i][0]\n",
    "    s = edgeList[i][1]\n",
    "    temp.append(f)\n",
    "    temp.append(s)\n",
    "    temp.append(2*node_emb_dict_2018[f]-node_emb_dict_2017[f])\n",
    "    temp.append(2*node_emb_dict_2018[s]-node_emb_dict_2017[s])\n",
    "    temp.append(1)\n",
    "    vel_pos_samp.append(temp)\n",
    "               \n",
    "vel_neg_samp = []\n",
    "for i in range(len(neg_set)):\n",
    "    temp = []\n",
    "    f = neg_set[i][0]\n",
    "    s = neg_set[i][1]\n",
    "    temp.append(f)\n",
    "    temp.append(s)\n",
    "    temp.append(2*node_emb_dict_2018[f]-node_emb_dict_2017[f])\n",
    "    temp.append(2*node_emb_dict_2018[s]-node_emb_dict_2017[s])\n",
    "    temp.append(-1)\n",
    "    vel_neg_samp.append(temp)  \n",
    "    \n",
    "vel_samp = []\n",
    "for i in vel_pos_samp:\n",
    "  vel_samp.append(i)\n",
    "for i in vel_neg_samp:\n",
    "  vel_samp.append(i)\n",
    "\n",
    "vel_shuf_samp = random.sample(vel_samp, len(vel_samp))\n",
    "vel_x = []\n",
    "vel_y = []\n",
    "for i in range(len(vel_shuf_samp)):\n",
    "    vel_x.append(np.concatenate((vel_shuf_samp[i][2], vel_shuf_samp[i][3]), axis=None))    \n",
    "    vel_y.append(vel_shuf_samp[i][4])\n",
    "    \n",
    "vel_xtrain, vel_xtest, vel_ytrain, vel_ytest = train_test_split(vel_x, vel_y, \n",
    "                                                test_size = 0.3, \n",
    "                                                random_state = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbd831-c620-4e9f-b848-1bb2076ba37d",
   "metadata": {},
   "source": [
    "### 3- Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f2368ad-1ff5-461c-8c3f-b3d7d8f55328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "0.6457894839470578\n",
      "velocity\n",
      "0.6963615936343718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "modelb = MLPClassifier(alpha=1, max_iter=1000)\n",
    "modelb.fit(loc_xtrain, loc_ytrain)\n",
    "ybase = modelb.predict(loc_xtest)\n",
    "modelv = MLPClassifier(alpha=1, max_iter=1000)\n",
    "modelv.fit(vel_xtrain, vel_ytrain)\n",
    "yvel = modelv.predict(vel_xtest)\n",
    "\n",
    "print(\"baseline\")\n",
    "print(sklearn.metrics.roc_auc_score(loc_ytest, ybase))\n",
    "\n",
    "print(\"velocity\")\n",
    "print(sklearn.metrics.roc_auc_score(vel_ytest, yvel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eeb3dae-bfe8-4f36-a115-fec6036cef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed2016 = embeddings[0]\n",
    "node_emb_dict_2016 = {list(SortedNodeG2019.nodes())[i] : embed2016[i] for i in range(len(SortedNodeG2019.nodes()))}\n",
    "\n",
    "# generate train/test data for acceleration\n",
    "\n",
    "acc_pos_samp = []\n",
    "for i in range(len(edgeList)):\n",
    "    temp = []\n",
    "    f = edgeList[i][0]\n",
    "    s = edgeList[i][1]\n",
    "    temp.append(f)\n",
    "    temp.append(s)\n",
    "    temp.append(2.25*node_emb_dict_2018[f]-1.5*node_emb_dict_2017[f]-0.25*node_emb_dict_2016[f])\n",
    "    temp.append(2.25*node_emb_dict_2018[s]-1.5*node_emb_dict_2017[s]-0.25*node_emb_dict_2016[s])\n",
    "    temp.append(1)\n",
    "    acc_pos_samp.append(temp)\n",
    "               \n",
    "acc_neg_samp = []\n",
    "for i in range(len(neg_set)):\n",
    "    temp = []\n",
    "    f = neg_set[i][0]\n",
    "    s = neg_set[i][1]\n",
    "    temp.append(f)\n",
    "    temp.append(s)\n",
    "    temp.append(2.25*node_emb_dict_2018[f]-1.5*node_emb_dict_2017[f]-0.25*node_emb_dict_2016[f])\n",
    "    temp.append(2.25*node_emb_dict_2018[s]-1.5*node_emb_dict_2017[s]-0.25*node_emb_dict_2016[s])\n",
    "    temp.append(-1)\n",
    "    acc_neg_samp.append(temp)  \n",
    "    \n",
    "acc_samp = []\n",
    "for i in acc_pos_samp:\n",
    "  acc_samp.append(i)\n",
    "for i in acc_neg_samp:\n",
    "  acc_samp.append(i)\n",
    "\n",
    "acc_shuf_samp = random.sample(acc_samp, len(acc_samp))\n",
    "acc_x = []\n",
    "acc_y = []\n",
    "for i in range(len(acc_shuf_samp)):\n",
    "    acc_x.append(np.concatenate((acc_shuf_samp[i][2], acc_shuf_samp[i][3]), axis=None))    \n",
    "    acc_y.append(acc_shuf_samp[i][4])\n",
    "    \n",
    "acc_xtrain, acc_xtest, acc_ytrain, acc_ytest = train_test_split(acc_x, acc_y, \n",
    "                                                test_size = 0.3, \n",
    "                                                random_state = 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55ae8412-69e2-4b12-924e-1b79fd091a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceleration\n",
      "0.6711056226888197\n"
     ]
    }
   ],
   "source": [
    "model_acc = MLPClassifier(alpha=1, max_iter=1000)\n",
    "model_acc.fit(acc_xtrain, acc_ytrain)\n",
    "y_acc = modelb.predict(acc_xtest)\n",
    "\n",
    "print(\"acceleration\")\n",
    "print(sklearn.metrics.roc_auc_score(acc_ytest, y_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715952e-2161-4cd0-89c4-a1db751f534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "x_acc = []\n",
    "y_acc = []\n",
    "for i in range(len(edgeList)):\n",
    "    f = edgeList[i][0]\n",
    "    s = edgeList[i][1]\n",
    "    x_acc.append(2.25*node_emb_dict_2018[f]-1.5*node_emb_dict_2017[f]-0.25*node_emb_dict_2016[f])\n",
    "    y_acc.append(2.25*node_emb_dict_2018[s]-1.5*node_emb_dict_2017[s]-0.25*node_emb_dict_2016[s])\n",
    "               \n",
    "acc_neg_samp = []\n",
    "for i in range(len(neg_set)):\n",
    "    f = neg_set[i][0]\n",
    "    s = neg_set[i][1]\n",
    "    x_acc.append(2.25*node_emb_dict_2018[f]-1.5*node_emb_dict_2017[f]-0.25*node_emb_dict_2016[f])\n",
    "    y_acc.append(2.25*node_emb_dict_2018[s]-1.5*node_emb_dict_2017[s]-0.25*node_emb_dict_2016[s]) \n",
    "\n",
    "acc_Similarities = cosine_similarity(x_acc, y_acc, dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd7d0fb7-de0d-4647-98b8-ce6d763ba7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed2016 = embeddings[0]\n",
    "node_emb_dict_2016 = {list(SortedNodeG2019.nodes())[i] : embed2016[i] for i in range(len(SortedNodeG2019.nodes()))}\n",
    "\n",
    "num_node = len(nodeList)\n",
    "\n",
    "vel_loc_2019 = []\n",
    "acc_loc_2019 = []\n",
    "\n",
    "for i in range(num_node):\n",
    "    node_name = nodeList[i]\n",
    "    vel_loc_2019.append(2*node_emb_dict_2018[node_name]-node_emb_dict_2017[node_name])\n",
    "    acc_loc_2019.append(2.25*node_emb_dict_2018[node_name]-1.5*node_emb_dict_2017[node_name]-0.25*node_emb_dict_2016[node_name])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f364e-2577-4504-a578-f4e4c8b9e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy\n",
    "\n",
    "total_size = num_node\n",
    "batch_size = 10\n",
    "batch_num = math.ceil(total_size/batch_size)\n",
    "start1 = 0\n",
    "start2 = 0\n",
    "\n",
    "acc_cosine_similarity = [[0 for x in range(num_node)] for y in range(num_node)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d72dc2-69c7-401d-8290-8394480b765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "acc_loc_2019 = np.asarray(acc_loc_2019, dtype=np.float32)\n",
    "\n",
    "start1 = 0\n",
    "start2 = 0\n",
    "\n",
    "for i in range(batch_num):\n",
    "    for j in range(batch_num):\n",
    "        acc_cosine_similarity[start1: start1 + batch_size][start2: start2 + batch_size] = \\\n",
    "        cosine_similarity(acc_loc_2019[start1: start1 + batch_size], \\\n",
    "                                                   acc_loc_2019[start2: start2 + batch_size], \\\n",
    "                          dense_output=True)\n",
    "        start2 = (j+1)*batch_size\n",
    "    start1 = (i+1)*batch_size\n",
    "    start2 = 0\n",
    "    if(i%10 == 0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3add0ec-8ab0-492d-b5ca-d826ab5f5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cosine_sim_file = open(\"acc_sim.txt\", \"w\")\n",
    "for row in acc_cosine_similarity:\n",
    "    np.savetxt(acc_cosine_sim_file, row)\n",
    "\n",
    "acc_cosine_sim_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e466307-98e9-4f1f-b0e7-f5b207d858ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = num_node\n",
    "batch_size = 10\n",
    "batch_num = math.ceil(total_size/batch_size)\n",
    "start = 0\n",
    "\n",
    "acc_loc_2019 = np.array(acc_loc_2019)\n",
    "batches = []\n",
    "for i in range(batch_num):\n",
    "    batches.append(acc_loc_2019[start: start+batch_size])\n",
    "    start = start+batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ca2b0-92c9-43bb-ab5b-f8e27bc69ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_batches = batches[:len(batches)-1]\n",
    "all_acc_sim = []\n",
    "count = 0\n",
    "for i in round_batches:\n",
    "    for j in round_batches:\n",
    "        all_acc_sim.append(cosine_similarity(i, j, dense_output=True))\n",
    "    if count%10 == 0:\n",
    "        print(count)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "21c3c388-829b-42b8-ab88-7938967e4a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.36765906,  0.99999964, -0.05642955,  0.05130933,  0.10695032,\n",
       "        0.07395042,  0.2655197 ,  0.1603888 ,  0.08726624,  0.17522533],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_acc_sim[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
